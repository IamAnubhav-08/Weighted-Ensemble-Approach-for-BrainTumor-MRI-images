{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import glob as glob\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import segmentation_models_3D as sm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dir = \"Evaluation/Test\"\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=16,\n",
    "    image_size=(256, 256),\n",
    "    seed=123,\n",
    "    shuffle=False,\n",
    "    interpolation='area'\n",
    ")\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = load_model(\"model_checkpoints/vgg19_cc_loss.hdf5\")\n",
    "densenet121 = load_model(\"model_checkpoints/densenet121_focal_loss.hdf5\", custom_objects={'focal_loss': focal_loss})\n",
    "mobilenetv2 = load_model(\"model_checkpoints/mobilenetv2_focal_loss.hdf5\",  custom_objects={'focal_loss': focal_loss})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 25s 512ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = vgg19.predict(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.7144428e-05, 3.3264086e-04, 5.8504579e-06, 9.9957436e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [vgg19, densenet121, mobilenetv2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  np.array([])\n",
    "for x, y in test_ds:\n",
    "  print(y.shape)\n",
    "  y_test = np.concatenate([y_test, y.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 8s 349ms/step\n",
      "25/25 [==============================] - 3s 116ms/step\n",
      "25/25 [==============================] - 1s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = [model.predict(test_ds) for model in models]\n",
    "preds = np.array(preds)\n",
    "weights = [0.4, 0.3, 0.3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 394, 4)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.0-cp310-cp310-win_amd64.whl (9.2 MB)\n",
      "     ---------------------------------------- 9.2/9.2 MB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\anubhav\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\anubhav\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "     -------------------------------------- 302.2/302.2 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.0 threadpoolctl-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\anubhav\\anaconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\anubhav\\anaconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\anubhav\\anaconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anubhav\\anaconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\anubhav\\anaconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\anubhav\\anaconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\anubhav\\anaconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anubhav\\anaconda3\\envs\\tf\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "weighted_preds = np.tensordot(preds, weights, axes=((0),(0)))\n",
    "weighted_ensemble_prediction = np.argmax(weighted_preds, axis=1)\n",
    "\n",
    "weighted_accuracy = accuracy_score(y_test, weighted_ensemble_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9873096446700508\n"
     ]
    }
   ],
   "source": [
    "print(weighted_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 4)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27411167512690354\n"
     ]
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 9s 351ms/step\n",
      "25/25 [==============================] - 3s 116ms/step\n",
      "25/25 [==============================] - 1s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction1 = vgg19.predict(test_ds)\n",
    "prediction1 = prediction1.argmax(axis=-1)\n",
    "\n",
    "prediction2 = densenet121.predict(test_ds)\n",
    "prediction2 = prediction2.argmax(axis=-1)\n",
    "\n",
    "prediction3 = mobilenetv2.predict(test_ds)\n",
    "prediction3 = prediction3.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1 = accuracy_score(y_test, prediction1)\n",
    "accuracy2 = accuracy_score(y_test, prediction2)\n",
    "accuracy3 = accuracy_score(y_test, prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9847715736040609\n",
      "0.9873096446700508\n",
      "0.9746192893401016\n"
     ]
    }
   ],
   "source": [
    "print(accuracy1)\n",
    "print(accuracy2)\n",
    "print(accuracy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Ensemble using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([])\n",
    "max_acc = []\n",
    "for w1 in range(0, 5):\n",
    "    for w2 in range(0,5):\n",
    "        for w3 in range(0,5):\n",
    "            wts = [w1/10.,w2/10.,w3/10.]\n",
    "            wted_preds1 = np.tensordot(preds, wts, axes=((0),(0)))\n",
    "            wted_ensemble_pred = np.argmax(wted_preds1, axis=1)\n",
    "            weighted_accuracy = accuracy_score(y_test, wted_ensemble_pred)\n",
    "            df2 = pd.DataFrame({'wts1':wts[0], 'wts2':wts[1], 'wts3':wts[2], 'acc':weighted_accuracy})\n",
    "            df.append(df2)\n",
    "            max_acc.append(weighted_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125,)\n"
     ]
    }
   ],
   "source": [
    "max_acc= np.array(max_acc)\n",
    "max_acc.shape\n",
    "prediction = np.argmax(max_acc)\n",
    "print(max_acc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9898477157360406\n"
     ]
    }
   ],
   "source": [
    "print(max_acc[prediction])\n",
    "\n",
    "# Test accuracy improved by 0.2% from the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
